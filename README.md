# This file contains the code snippets I created as a result of the Miuul Feature Engineering Training. The topics covered in the training content are as follows:

Feature engineering is an important concept in the fields of machine learning and data analytics. In this process, we strive to extract meaningful and useful features from a dataset. 
These features enable a model to learn better and ultimately lead to better predictions.
Feature engineering encompasses various techniques such as transforming variables in the dataset, creating new features, and selecting relevant features. 
For instance, we can improve the distribution of a numerical variable by taking its logarithm or create a new interactive feature by combining two variables.

Outliers(Outlier Detection and Handling): Outliers are values in a dataset that are significantly different from other examples and can influence data analysis. 
Outliers should be considered and appropriately addressed in the feature engineering process. Various methods can be used to detect and handle outliers, such as identifying them and correcting or removing them.

Missing Values(Missing Data Analysis and Handling): Missing values in a dataset refer to empty or blank cells in one or more variables. These missing values can pose a problem in the feature engineering process because 
some algorithms do not accept missing values or may misinterpret them. Various strategies can be employed to handle missing values, such as filling them with the mean, median, or nearest neighbor values.

Encoding (Categorical Variable Encoding): Encoding involves transforming categorical variables in a dataset into numerical values that can be used in machine learning algorithms. Machine learning models prefer to work with numerical data. 
Therefore, categorical variables need to be encoded or transformed. This process can be accomplished using techniques like one-hot encoding, label encoding, or ordinal encoding.

Scaling: In the feature engineering process, it is common to encounter different features with different scales or ranges. This situation can lead to some algorithms giving more weight to features with larger scales. 
To address this, scaling techniques are used to bring features to similar scales. For example, standardization or normalization methods can be employed to make features have a mean of zero, 
a variance of one, or a specific range.


![FE](https://github.com/busraCin/FeatureEngineering/assets/69642923/6e013930-4de4-4813-a675-e38516251451)
